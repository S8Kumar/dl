{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **43177 STUTI KUMAR**"
      ],
      "metadata": {
        "id": "kpLIekzW7gs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A6 - Transfer Learning**"
      ],
      "metadata": {
        "id": "ZCYrggK97iBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "_BsRhZk582fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3rUl2xo7ZS9"
      },
      "outputs": [],
      "source": [
        "# Image transformations\n",
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ]),\n",
        "    # Validation does not use augmentation\n",
        "    'valid':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets from folders\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n",
        "    'valid':\n",
        "    datasets.ImageFolder(root=validdir, transform=image_transforms['valid']),\n",
        "}\n",
        "\n",
        "# Dataloader iterators, make sure to shuffle\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['valid'], batch_size=batch_size, shuffle=True)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "dBNofT2c84gN",
        "outputId": "b66aaca0-ef8e-43e8-cf5f-65a4a135ee78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-33a5aeb811ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m data = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraindir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m'valid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvaliddir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'traindir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataloader once\n",
        "trainiter = iter(dataloaders['train'])\n",
        "features, labels = next(trainiter)\n",
        "features.shape, labels.shape"
      ],
      "metadata": {
        "id": "CHsE24ef9Kwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "4sYDqQ5c9LQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze model weights\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "UKa074oo9Tzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add on classifier\n",
        "model.classifier[6] = nn.Sequential(\n",
        "                      nn.Linear(n_inputs, 256), \n",
        "                      nn.ReLU(), \n",
        "                      nn.Dropout(0.4),\n",
        "                      nn.Linear(256, n_classes),                   \n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "metadata": {
        "id": "whcKqnm-9WZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only training classifier[6]\n",
        "model.classifier\n",
        "Sequential(\n",
        "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
        "  (1): ReLU(inplace)\n",
        "  (2): Dropout(p=0.5)\n",
        "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "  (4): ReLU(inplace)\n",
        "  (5): Dropout(p=0.5)\n",
        "  (6): Sequential(\n",
        "    (0): Linear(in_features=4096, out_features=256, bias=True)\n",
        "    (1): ReLU()\n",
        "    (2): Dropout(p=0.4)\n",
        "    (3): Linear(in_features=256, out_features=100, bias=True)\n",
        "    (4): LogSoftmax()\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "r9X7l38L9hnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find total parameters and trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'{total_params:,} total parameters.')\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'{total_trainable_params:,} training parameters.')"
      ],
      "metadata": {
        "id": "sQy-aTJ-9m7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criteration = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "rneMl8Rb9z25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  for data, targets in trainloader:\n",
        "    # Generate predictions\n",
        "    out = model(data)\n",
        "    # Calculate loss\n",
        "    loss = criterion(out, targets)\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    # Update model parameters\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "XhsNvOTQ953n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping details\n",
        "n_epochs_stop = 5\n",
        "min_val_loss = np.Inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Main loop\n",
        "for epoch in range(n_epochs):\n",
        "  # Initialize validation loss for epoch\n",
        "  val_loss = 0\n",
        "  \n",
        "  # Training loop\n",
        "  for data, targets in trainloader:\n",
        "    # Generate predictions\n",
        "    out = model(data)\n",
        "    # Calculate loss\n",
        "    loss = criterion(out, targets)\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    # Update model parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "# Validation loop\n",
        "for data, targets in validloader:\n",
        "  # Generate predictions \n",
        "  out = model(data)\n",
        "  # Calculate loss\n",
        "  loss = criterion(out, targets)\n",
        "  val_loss += loss\n",
        "\n",
        "# Average validation loss\n",
        "val_loss = val_loss / len(trainloader)\n",
        "\n",
        "# If the validation loss is at a minimum\n",
        "if val_loss < min_val_loss:\n",
        "  # Save the model\n",
        "  torch.save(model, checkpoint_path)\n",
        "  epochs_no_improve = 0\n",
        "  min_val_loss = val_loss\n",
        "  \n",
        "else:\n",
        "  epochs_no_improve += 1\n",
        "  # Check early stopping condition\n",
        "  if epochs_no_improve == n_epochs_stop:\n",
        "    print('Early stopping!')\n",
        "    \n",
        "    # Load in the best model\n",
        "    model = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "cfedNy9-97CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, targets in testloader:\n",
        "    log_ps = model(data)\n",
        "    # Convert to probabilities\n",
        "    ps = torch.exp(log_ps)\n",
        "ps.shape()"
      ],
      "metadata": {
        "id": "FMS-ApET-FrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find predictions and correct\n",
        "pred = torch.max(ps, dim=1)\n",
        "equals = pred == targets\n",
        "# Calculate accuracy\n",
        "accuracy = torch.mean(equals)"
      ],
      "metadata": {
        "id": "BF1I3XBv-H7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_ps, top_5_classes = ps.topk(5, dim=1)\n",
        "top_5_ps.shape"
      ],
      "metadata": {
        "id": "DyzYCTV6-Ij3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnR26NAa-NEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}