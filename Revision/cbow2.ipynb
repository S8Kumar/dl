{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils, pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Lambda\n",
    "from keras.optimizers import SGD, Adam, Adadelta\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('chernobyl.txt','r')\n",
    "text = [text for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "word2id = tokenizer.word_index\n",
    "wids = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "vocablen = len(word2id)\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "\n",
    "embeddingsize = 100\n",
    "windowsize =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_words(corpus, windowsize, vocablen):\n",
    "    windowlen = windowsize*2\n",
    "    for words in corpus:\n",
    "        wordlen = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context = []\n",
    "            labels = []\n",
    "\n",
    "            start = index - windowsize\n",
    "            end = index +  windowsize +1\n",
    "\n",
    "            context.append([words[i] for i in range(start,end) if 0<= i < windowlen & i!= index])\n",
    "            labels.append(word)\n",
    "\n",
    "            x = pad_sequences(context,maxlen=windowlen)\n",
    "            y = np_utils.to_categorical(labels, vocablen)\n",
    "\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocablen, output_dim=embeddingsize, input_length=windowsize*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x,axis=1), output_shape=(embeddingsize, )))\n",
    "cbow.add(Dense(vocablen, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            23600     \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 236)               23836     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,436\n",
      "Trainable params: 47,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2043.1896319389343\n",
      "Epoch: 2 Loss: 2043.1250319480896\n",
      "Epoch: 3 Loss: 2043.0783619880676\n",
      "Epoch: 4 Loss: 2043.0311675071716\n",
      "Epoch: 5 Loss: 2042.9836502075195\n",
      "Epoch: 6 Loss: 2042.935945034027\n",
      "Epoch: 7 Loss: 2042.8880977630615\n",
      "Epoch: 8 Loss: 2042.8401670455933\n",
      "Epoch: 9 Loss: 2042.7921466827393\n",
      "Epoch: 10 Loss: 2042.744077205658\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss =0\n",
    "    for x,y in generate_context_words(wids, windowsize, vocablen):\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236, 100)\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236, 236)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distmat = euclidean_distances(weights)\n",
    "distmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chernobyl': ['zone', 'russian', 'be', 'despite', 'still']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = {search_term: [ id2word[idx] for idx in distmat[word2id[search_term]-1].argsort()[1:6] +1]\n",
    "    for search_term in ['chernobyl']}\n",
    "\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cb152312354fdf8e65d4c58beb131d8b6cf53bf6c2300780a8c702780ef5ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
